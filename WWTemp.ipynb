{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "2020-10-07T12:00:06-07:00\n",
      "\n",
      "CPython 3.7.4\n",
      "IPython 7.17.0\n",
      "\n",
      "compiler   : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 17.7.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 12\n",
      "interpreter: 64bit\n",
      "\n",
      "\n",
      "tensorflow  2.1.0\n",
      "keras  2.2.4-tf\n",
      "torch  1.6.0\n"
     ]
    }
   ],
   "source": [
    "import sys, os \n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "\n",
    "#import tensorflow.keras.models.load_model\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark\n",
    "print(\"\\n\")\n",
    "print(\"tensorflow \",tf.__version__)\n",
    "print(\"keras \",keras.__version__)\n",
    "print(\"torch \",torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Test Models\n",
    "\n",
    "- pyTorch: vgg\n",
    "- keras:  vgg\n",
    "- huggingface: gpt_x\n",
    "- other ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem:  VERY OLD API\n",
    "\n",
    "# load vgg model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "# load the model\n",
    "vgg16_keras = VGG16()\n",
    "vgg19_keras = VGG19()\n",
    "\n",
    "# summarize the model\n",
    "# model.summary()\n",
    "\n",
    "#deprecated \n",
    "#from pytorchcv.model_provider import get_model as ptcv_get_model\n",
    "#vgg16_pytorch = ptcv_get_model('vgg16', pretrained=True)\n",
    "import torchvision\n",
    "vgg16_pytorch = torchvision.models.vgg16(pretrained=True)\n",
    "vgg16_pytorch_init = torchvision.models.vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs\n",
    "\n",
    "https://keras.io/api/layers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntFlag, auto, Enum\n",
    "\n",
    "class LAYER_TYPE(IntFlag):\n",
    "    UNKNOWN = auto()\n",
    "    DENSE = auto()\n",
    "    CONV1D = auto()\n",
    "    CONV2D = auto()\n",
    "    FLATTENED = auto()\n",
    "    EMBEDDING = auto()\n",
    "    NORM = auto()\n",
    "\n",
    "    \n",
    "class FRAMEWORK(IntFlag):\n",
    "    UNKNOWN = auto()\n",
    "    PYTORCH = auto()\n",
    "    KERAS = auto()\n",
    "    \n",
    "class FRAMEWORK(IntFlag):\n",
    "    UNKNOWN = auto()\n",
    "    PYTORCH = auto()\n",
    "    KERAS = auto()\n",
    "    \n",
    "class CHANNELS(IntFlag):\n",
    "    FIRST = auto()\n",
    "    LAST = auto()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " def get_layer(self):\n",
    "        return self.layer\n",
    "    \n",
    "    def get_index(self):\n",
    "        return self.inded\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "    \n",
    "    #\n",
    "    # original weight tensors\n",
    "    #\n",
    "    def set_has_weights(tf):\n",
    "        self.has_weights = tf\n",
    "        \n",
    "    def has_weights():\n",
    "        return self.has_weights\n",
    "    \n",
    "    def set_weights(w):\n",
    "        self.weights = w\n",
    "        \n",
    "    def get_weights():\n",
    "        return self.weights\n",
    "    \n",
    "       \n",
    "    #    \n",
    "    # extracted weight matrices    \n",
    "    #\n",
    "    def set_Wmats(Wmats):\n",
    "        self.Wmats = Wmats\n",
    "        \n",
    "    def get_Wmats(self):\n",
    "        return self.Wmats\n",
    "    \n",
    "    \n",
    "    def set_N(N):\n",
    "        self.N = N\n",
    "        \n",
    "    def get_N():\n",
    "        return self.N\n",
    "    \n",
    "    def set_M(N):\n",
    "        self.M = M\n",
    "        \n",
    "    def get_M():\n",
    "        return self.M\n",
    "    \n",
    "    def set_receptive_field_size(rf):\n",
    "        self.receptive_field_size = rf\n",
    "        \n",
    "    def get_receptive_field_size():\n",
    "        return self.receptive_field_size \n",
    "    \n",
    "    def set_inputs_shape(shape):\n",
    "        self.inputs_shape = shape\n",
    "        \n",
    "    def gst_inputs_shape(self):\n",
    "        return self.inputs_shape\n",
    "    \n",
    "    def set_outputs_shape(shape):\n",
    "        self.outputs_shape = shape\n",
    "    \n",
    "    def get_outputs_shape(self):\n",
    "        return self.outputs_shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    def num_W(self):\n",
    "        return self.num_W\n",
    "    \n",
    "    def get_W(k=0):\n",
    "        return self.Wmats[k]\n",
    "        \n",
    "    def get_inputs_shape(self):\n",
    "        return self.inputs_shape\n",
    "    \n",
    "    def get_outputs_shape(self):\n",
    "        return self.outputs_shape\n",
    "    \n",
    "    def is_skipped(self):\n",
    "        return self.skipped\n",
    "    \n",
    "    def get_type(self):\n",
    "        return this.type\n",
    " \n",
    "    def get_framework(self):\n",
    "        return self.framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I extend / annotate the layer to add more elements\n",
    "\n",
    "- base class -> child ?  ugh\n",
    "- mixin\n",
    "- wrapper class that is a class (I forget the pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WWLayer:\n",
    "    def __init__(self, layer, index=-1, name=\"\", \n",
    "                 the_type=LAYER_TYPE.UNKNOWN, framework=FRAMEWORK.UNKNOWN, skipped=False):\n",
    "        self.layer = layer\n",
    "        self.index = index\n",
    "        self.name = name\n",
    "        self.skipped = skipped\n",
    "        self.the_type = the_type\n",
    "        self.framework = framework\n",
    "        \n",
    "        if (self.framework==FRAMEWORK.KERAS):\n",
    "             self.channels = CHANNELS.FIRST\n",
    "        elif (self.framework==FRAMEWORK.PYTORCH):\n",
    "            self.channels = CHANNELS.LAST\n",
    "        \n",
    "        # orginal weights and biases\n",
    "        self.has_weights = False\n",
    "        self.weights = None\n",
    "        \n",
    "        self.has_biases = False\n",
    "        self.biases = None\n",
    "        \n",
    "\n",
    "        \n",
    "        # extracted weight matrices\n",
    "        self.num_W = 0\n",
    "        self.Wmats = []\n",
    "        self.N = 0\n",
    "        self.M = 0\n",
    "        self.num_components = self.M\n",
    "        self.receptive_field_size = None\n",
    "        self.inputs_shape = []\n",
    "        self.outputs_shape = []\n",
    "        \n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"WWLayer()\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"WWLayer {}  {} {} {}  skipped {}\".format(self.index, self.name, \n",
    "                                                       self.framework.name, self.the_type.name, self.skipped)\n",
    "        \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_iter(model):\n",
    "    layer_iter = None\n",
    "    \n",
    "    if hasattr(model, 'layers'):\n",
    "        layer_iter = (l for l in model.layers)\n",
    "        framework = FRAMEWORK.KERAS\n",
    "    elif hasattr(model, 'modules'):\n",
    "        layer_iter = model.modules()\n",
    "        framework = FRAMEWORK.PYTORCH\n",
    "    else:\n",
    "        layer_iter = None\n",
    "        framework = FRAMEWORK.UNKNOWN\n",
    "        \n",
    "    return layer_iter, framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_weights(ww_layer):\n",
    "    \"\"\"obtain the orginal weights and biases for the layer, if present\"\"\"\n",
    "    \n",
    "    has_weights, has_biases = False, False\n",
    "    weights, biases = None, None\n",
    "    receptive_field_size = 0\n",
    "    \n",
    "    l = ww_layer.layer\n",
    "      \n",
    "    if ww_layer.framework==FRAMEWORK.PYTORCH:\n",
    "        if hasattr(l, 'weight'):\n",
    "            w = [np.array(l.weight.data.clone().cpu())]\n",
    "            has_weights = True\n",
    "            \n",
    "    elif ww_layer.framework==FRAMEWORK.KERAS:\n",
    "        w = l.get_weights()\n",
    "        if(len(w)>0):\n",
    "            has_weights = True\n",
    "            \n",
    "        if(len(w)>1):\n",
    "            has_biases = True\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        print(\"unknown framework\")\n",
    "        assert(False)\n",
    "   \n",
    "    if has_weights:\n",
    "        if len(w)==1:\n",
    "            weights = w[0]\n",
    "            biases = None\n",
    "        elif len(w)==2:\n",
    "            weights = w[0]\n",
    "            biases =  w[1]\n",
    "        else:\n",
    "            print(\"unknown weight shape\")\n",
    "            assert(False)\n",
    "        \n",
    "    \n",
    "    #receptive_field_size = l.weight.data[0][0].numel()\n",
    "    #            else:\n",
    "    \n",
    "    return has_weights, weights, has_biases, biases        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# TODO:  maybe change to use channels.first / channels.last \n",
    "\n",
    "def get_conv2D_Wmats(Wtensor):\n",
    "        \"\"\"Extract W slices from a 4 index conv2D tensor of shape: (N,M,i,j) or (M,N,i,j).  \n",
    "        Return ij (N x M) matrices\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #self.info(\"get_conv2D_Wmats\")\n",
    "\n",
    "        Wmats = []\n",
    "        s = Wtensor.shape\n",
    "        N, M, imax, jmax = s[0],s[1],s[2],s[3]\n",
    "        if N + M >= imax + jmax:\n",
    "            #self.debug(\"Pytorch tensor shape detected: {}x{} (NxM), {}x{} (i,j)\".format(N, M, imax, jmax))\n",
    "            \n",
    "            for i in range(imax):\n",
    "                for j in range(jmax):\n",
    "                    W = Wtensor[:,:,i,j]\n",
    "                    if N < M:\n",
    "                        W = W.T\n",
    "                    Wmats.append(W)\n",
    "        else:\n",
    "            N, M, imax, jmax = imax, jmax, N, M          \n",
    "            #self.debug(\"Tf.Keras.tensor shape detected: {}x{} (NxM), {}x{} (i,j)\".format(N, M, imax, jmax))\n",
    "            \n",
    "            for i in range(imax):\n",
    "                for j in range(jmax):\n",
    "                    W = Wtensor[i,j,:,:]\n",
    "                    if N < M:\n",
    "                        W = W.T\n",
    "                    Wmats.append(W)\n",
    "                    \n",
    "        #self.info(\"get_conv2D_Wmats N={} M={}\".format(N,M))\n",
    "\n",
    "        rf = imax*jmax # receptive field size \n",
    "        return Wmats, N, M, rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weight_matrices(ww_layer, conv2d_fft=False, conv2d_norm=True):\n",
    "    \"\"\"extract the weight matrices from the layer weights (tensors)\n",
    "    sets the weights on the ww_layer\n",
    "    \n",
    "        conv2d_fft not supported yet \"\"\"\n",
    "    \n",
    "    if not ww_layer.has_weights:\n",
    "        # throw exception ?\n",
    "        return \n",
    "        \n",
    "    weights = ww_layer.weights\n",
    "    the_type = ww_layer.the_type\n",
    "    N, M, n_comp, rf = 0, 0, 0, None\n",
    "    Wmats = []\n",
    "    \n",
    "    # this may change if we treat Conv1D differently layer\n",
    "    if (the_type == LAYER_TYPE.DENSE or the_type == LAYER_TYPE.CONV1D):\n",
    "        Wmats = [weights]\n",
    "        N, M = np.max(weights.shape), np.min(weights.shape)\n",
    "        n_comp = M\n",
    "        \n",
    "    elif the_type == LAYER_TYPE.CONV2D:\n",
    "        Wmats, N, M, rf = get_conv2D_Wmats(weights)\n",
    "        n_comp = M\n",
    "        \n",
    "    elif the_type == LAYER_TYPE.NORM:\n",
    "        pass#print(\"Layer norm has no matrices\")\n",
    "    \n",
    "    else:\n",
    "        print(\"unknown type {} layer  {}\".format(the_type, ww_layer.layer))\n",
    "\n",
    "\n",
    "    ww_layer.N = N\n",
    "    ww_layer.M = M\n",
    "    ww_layer.num_components = n_comp\n",
    "    ww_layer.receptive_field_size = rf\n",
    "    ww_layer.Wmats = Wmats\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_type_list(filename):\n",
    "    \"\"\"Read a list of layer types\"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_type(layer):\n",
    "    \"\"\"Detemine layer type\"\"\"\n",
    "    \n",
    "    the_type = LAYER_TYPE.UNKNOWN\n",
    "    typestr = (str(type(layer))).lower()\n",
    "        \n",
    "    # Keras TF 2.x types\n",
    "    if isinstance(layer, tf.keras.layers.Dense): \n",
    "        the_type = LAYER_TYPE.DENSE\n",
    "        \n",
    "    elif isinstance(layer, keras.layers.Conv1D):                \n",
    "        the_type = LAYER_TYPE.CONV1D\n",
    "    \n",
    "    elif isinstance(layer, keras.layers.Conv2D):                \n",
    "        the_type = LAYER_TYPE.CONV2D\n",
    "        \n",
    "    elif isinstance(layer, tf.keras.layers.Flatten):\n",
    "        the_type = LAYER_TYPE.FLATTENED\n",
    "        \n",
    "    elif isinstance(layer, tf.keras.layers.Embedding):\n",
    "        the_type = LAYER_TYPE.EMBEDDING\n",
    "        \n",
    "    elif isinstance(layer, tf.keras.layers.LayerNormalization):\n",
    "        the_type = LAYER_TYPE.NORM\n",
    "        \n",
    "        \n",
    " \n",
    "    # PyTorch\n",
    "    \n",
    "         \n",
    "    elif isinstance(layer, torch.nn.Linear):\n",
    "        the_type = LAYER_TYPE.DENSE\n",
    "        \n",
    "    elif isinstance(layer, torch.nn.Conv1d):\n",
    "        the_type = LAYER_TYPE.CONV1D\n",
    "\n",
    "    elif isinstance(layer, torch.nn.Conv2d):\n",
    "        the_type = LAYER_TYPE.CONV2D\n",
    "        \n",
    "    elif isinstance(layer, torch.nn.Embedding):\n",
    "        the_type = LAYER_TYPE.EMBEDDING\n",
    "            \n",
    "    elif isinstance(layer, torch.nn.LayerNorm):\n",
    "        the_type = LAYER_TYPE.NORM\n",
    "        \n",
    "\n",
    "    # try to infer type (i.e for huggingface)\n",
    "    elif typestr.endswith(\".linear'>\"):\n",
    "        the_type = LAYER_TYPE.DENSE\n",
    "        \n",
    "    elif typestr.endswith(\".dense'>\"):\n",
    "        the_type = LAYER_TYPE.DENSE\n",
    "        \n",
    "    elif typestr.endswith(\".conv1d'>\"):\n",
    "        the_type = LAYER_TYPE.CONV1D\n",
    "        \n",
    "    elif typestr.endswith(\".conv2d'>\"):\n",
    "        the_type = LAYER_TYPE.CONV2D\n",
    "\n",
    "    return the_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ww_layer(layer, index, filter_ids=None, filter_types=None, framework=None):\n",
    "    \"\"\"Make ww_layer or return None if layer is skipped\"\"\"\n",
    "    \n",
    "    skipped = False\n",
    "    the_type = layer_type(layer)\n",
    "    name = \"\"\n",
    "    has_weights = False;\n",
    "    \n",
    "    if hasattr(layer, 'name'):\n",
    "        name = layer.name\n",
    "    \n",
    "    if filter_ids is not None and len(filter_ids) > 0:\n",
    "        if layer_id not in filter_ids:\n",
    "            skipped = True\n",
    "            \n",
    "    if filter_types is not None and len(filter_types) > 0:\n",
    "        if the_type not in filter_types:\n",
    "            skipped = True\n",
    "                                    \n",
    "    \n",
    "    ww_layer = WWLayer(layer, index=index, name=name, \n",
    "                 the_type=the_type, framework=framework, skipped=skipped)\n",
    "    \n",
    "    has_weights, weights, has_biases, biases = layer_weights(ww_layer)\n",
    "    \n",
    "    \n",
    "    ww_layer.has_weights = has_weights\n",
    "    ww_layer.has_biases = has_biases\n",
    "    \n",
    "    if has_biases:\n",
    "        ww_layer.biases = biases   \n",
    "        \n",
    "    if has_weights:    \n",
    "        ww_layer.weights = weights\n",
    "        set_weight_matrices(ww_layer)\n",
    "\n",
    "    \n",
    "    return ww_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WWLayerIterator:\n",
    "\n",
    "    \"\"\"Iterator that loops over layers, with matrices available.\"\"\"\n",
    "\n",
    "    def __init__(self, model, filter_ids=[], filter_types=[]):\n",
    "        self.model = model\n",
    "        self.layers_iter, self.framework = model_iter(model) \n",
    "\n",
    "        self.filter_ids = filter_ids\n",
    "        self.filter_types = filter_types\n",
    "        \n",
    "        self.k = 0\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    # Python 3 compatibility\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        curr_layer = next(self.layers_iter)\n",
    "        if curr_layer:    \n",
    "            curr_id, self.k = self.k, self.k+1\n",
    "            #curr_layer = self.layers[curr_id]\n",
    "            ww_layer = make_ww_layer(curr_layer, curr_id, \n",
    "                                     filter_ids=self.filter_ids, \n",
    "                                     filter_types=self.filter_types, \n",
    "                                     framework=self.framework)\n",
    "                        \n",
    "            return ww_layer\n",
    "        else:\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with different frameworks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWLayer 0  input_1 KERAS UNKNOWN  skipped False\n",
      "W : (3, 3, 3, 64)\n",
      "b  (64,)\n",
      ".......\n",
      "W : (3, 3, 64, 64)\n",
      "b  (64,)\n",
      ".......\n",
      ".......\n",
      "W : (3, 3, 64, 128)\n",
      "b  (128,)\n",
      ".......\n",
      "W : (3, 3, 128, 128)\n",
      "b  (128,)\n",
      ".......\n",
      ".......\n",
      "W : (3, 3, 128, 256)\n",
      "b  (256,)\n",
      ".......\n",
      "W : (3, 3, 256, 256)\n",
      "b  (256,)\n",
      ".......\n",
      "W : (3, 3, 256, 256)\n",
      "b  (256,)\n",
      ".......\n",
      ".......\n",
      "W : (3, 3, 256, 512)\n",
      "b  (512,)\n",
      ".......\n",
      "W : (3, 3, 512, 512)\n",
      "b  (512,)\n",
      ".......\n",
      "W : (3, 3, 512, 512)\n",
      "b  (512,)\n",
      ".......\n",
      ".......\n",
      "W : (3, 3, 512, 512)\n",
      "b  (512,)\n",
      ".......\n",
      "W : (3, 3, 512, 512)\n",
      "b  (512,)\n",
      ".......\n",
      "W : (3, 3, 512, 512)\n",
      "b  (512,)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (25088, 4096)\n",
      "b  (4096,)\n",
      ".......\n",
      "W : (4096, 4096)\n",
      "b  (4096,)\n",
      ".......\n",
      "W : (4096, 1000)\n",
      "b  (1000,)\n",
      ".......\n"
     ]
    }
   ],
   "source": [
    "ww_layer_iterator = WWLayerIterator(vgg16_keras)\n",
    "\n",
    "for ww_layer in ww_layer_iterator:\n",
    "    print(ww_layer)\n",
    "    for ww_layer in ww_layer_iterator:\n",
    "    #    print(ww_layer)\n",
    "        if(ww_layer.has_weights):\n",
    "            print(\"W :\",ww_layer.weights.shape)\n",
    "        if(ww_layer.has_biases):\n",
    "            print(\"b \",ww_layer.biases.shape)\n",
    "        print(\".......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWLayer 0   PYTORCH UNKNOWN  skipped False\n",
      ".......\n",
      "W : (64, 3, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (64, 64, 3, 3)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (128, 64, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (128, 128, 3, 3)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (256, 128, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (256, 256, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (256, 256, 3, 3)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (512, 256, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (512, 512, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (512, 512, 3, 3)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (512, 512, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (512, 512, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (512, 512, 3, 3)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (4096, 25088)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (4096, 4096)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (1000, 4096)\n",
      ".......\n"
     ]
    }
   ],
   "source": [
    "ww_layer_iterator = WWLayerIterator(vgg16_pytorch)\n",
    "\n",
    "for ww_layer in ww_layer_iterator:\n",
    "    print(ww_layer)\n",
    "    for ww_layer in ww_layer_iterator:\n",
    "    #    print(ww_layer)\n",
    "        if(ww_layer.has_weights):\n",
    "            print(\"W :\",ww_layer.weights.shape)\n",
    "        if(ww_layer.has_biases):\n",
    "            print(\"b \",ww_layer.biases.shape)\n",
    "        print(\".......\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      ".......\n",
      "unknown type 32 layer  Embedding(50257, 768)\n",
      "W : (50257, 768)\n",
      ".......\n",
      "unknown type 32 layer  Embedding(1024, 768)\n",
      "W : (1024, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      "W : (50257, 768)\n",
      ".......\n"
     ]
    }
   ],
   "source": [
    "from transformers import  GPT2LMHeadModel\n",
    "gpt2_lmh = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "ww_layer_iterator = WWLayerIterator(gpt2_lmh)\n",
    "for ww_layer in ww_layer_iterator:\n",
    "#    print(ww_layer)\n",
    "    if(ww_layer.has_weights):\n",
    "        print(\"W :\",ww_layer.weights.shape)\n",
    "    if(ww_layer.has_biases):\n",
    "        print(\"b \",ww_layer.biases.shape)\n",
    "    print(\".......\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 64) (64,)\n",
      "(3, 3, 64, 64) (64,)\n",
      "(3, 3, 64, 128) (128,)\n",
      "(3, 3, 128, 128) (128,)\n",
      "(3, 3, 128, 256) (256,)\n",
      "(3, 3, 256, 256) (256,)\n",
      "(3, 3, 256, 256) (256,)\n",
      "(3, 3, 256, 512) (512,)\n",
      "(3, 3, 512, 512) (512,)\n",
      "(3, 3, 512, 512) (512,)\n",
      "(3, 3, 512, 512) (512,)\n",
      "(3, 3, 512, 512) (512,)\n",
      "(3, 3, 512, 512) (512,)\n",
      "(25088, 4096) (4096,)\n",
      "(4096, 4096) (4096,)\n",
      "(4096, 1000) (1000,)\n"
     ]
    }
   ],
   "source": [
    "layer_iterator = WWLayerIterator(vgg16_keras)\n",
    "\n",
    "for ww_layer in layer_iterator:\n",
    "    if(ww_layer.has_weights):\n",
    "        print(ww_layer.weights.shape, ww_layer.biases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- set a few basics for keras and tf, then infer the rest by name\n",
    "- allow a yaml that people can edit with lists of others\n",
    "\n",
    "\n",
    "\n",
    "- set rf, M, N\n",
    "- set orig tensor ?\n",
    "- be able to reset weights => layer iterator allows get and set\n",
    "- try to get inputs , outputs for both frameworks\n",
    "- test attention model => need to label properly the attention layer matrices\n",
    "- extract combined eigenvalues for Conv2D\n",
    "- extract matrices for attention\n",
    "- RNN, LSTM, ... (pretrained from where)\n",
    "- be able to take W-Wdiff using original tensor, not slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:  Wednesday++\n",
    "\n",
    "- write wwcomparator: use iterator to compare |W-Winit|\n",
    "- write wwregularizer:  use iterator to compute SVD, output new model\n",
    "- rewrite ww:  compute SVD, ESD, supporting metrics, etc\n",
    "\n",
    "#### Python Lint s\n",
    "- clean nup documentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used\n",
    "\n",
    "def same_models(model_1, model_2):\n",
    "    \"\"\"Compare models to see if the are the same architecture\"\"\"\n",
    "    \n",
    "    same = True\n",
    "    layer_iter_1 = WWLayerIterator(model_1)\n",
    "    layer_iter_2 = WWLayerIterator(model_2)\n",
    "    \n",
    "    same = layer_iter_1.framework == layer_iter_2.framework \n",
    "\n",
    "  \n",
    "    return same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances(model_1, model_2):\n",
    "    \"\"\"Compute the distances between model_1 and model_2 for each layer. \n",
    "    Reports Frobenius norm of the distance between each layer weights (tensor)\n",
    "    \n",
    "       < ||W_1-W_2|| >\n",
    "       \n",
    "    output: avg delta W, a details dataframe\n",
    "       \n",
    "    models should be the same size and from the same framework\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    # check and throw exception if inputs incorrect\n",
    "    # TODO: review design here...may need something else\n",
    "    #   need to:\n",
    "    #.   - iterate over all layers and check\n",
    "    #.   - inspect framework by framework\n",
    "    #.   - check here instead\n",
    "    #\n",
    "    \n",
    "    same = True\n",
    "    layer_iter_1 = WWLayerIterator(model_1)\n",
    "    layer_iter_2 = WWLayerIterator(model_2)\n",
    "    \n",
    "    same = layer_iter_1.framework == layer_iter_2.framework \n",
    "    if not same:\n",
    "        raise Exception(\"Sorry, models are from different frameworks\")\n",
    "        \n",
    "   \n",
    "    \n",
    "    details = pd.DataFrame(columns = ['layer_id', 'name', 'delta_W', 'delta_b', 'W_shape', 'b_shape'])\n",
    "    data = {}\n",
    "    \n",
    "    try:      \n",
    "        for layer_1, layer_2 in zip(layer_iter_1, layer_iter_2):\n",
    "            data['layer_id'] = layer_1.index\n",
    "            data['name'] = layer_1.name\n",
    "\n",
    "            if layer_1.has_weights:\n",
    "                data['delta_W'] = np.linalg.norm(layer_1.weights-layer_2.weights)\n",
    "                data['W_shape'] = layer_1.weights.shape\n",
    "\n",
    "                if layer_1.has_biases:\n",
    "                    data['delta_b'] = np.linalg.norm(layer_1.biases-layer_2.biases)\n",
    "                    data['b_shape'] = layer_1.biases.shape\n",
    "\n",
    "                details = details.append(data,  ignore_index=True)\n",
    "    except:\n",
    "        raise Exception(\"Sorry, problem comparing models\")\n",
    "    \n",
    "    details.set_index('layer_id', inplace=True)\n",
    "    avg_dW = np.mean(details['delta_W'].to_numpy())\n",
    "    return avg_dW, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dW, details = distances(vgg16_keras, vgg16_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dW, details = distances(vgg16_pytorch, vgg16_pytorch_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>delta_W</th>\n",
       "      <th>delta_b</th>\n",
       "      <th>W_shape</th>\n",
       "      <th>b_shape</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>10.686150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(64, 3, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>15.589570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>17.629187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(128, 64, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>21.771776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>23.527433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(256, 128, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>29.479052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>29.830938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>32.601643</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 256, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>40.713379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>40.664497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td>41.603882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td>41.327408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td>40.439659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td>113.576752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(4096, 25088)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td></td>\n",
       "      <td>58.012165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(4096, 4096)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td></td>\n",
       "      <td>42.508610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(1000, 4096)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name     delta_W  delta_b           W_shape  b_shape\n",
       "layer_id                                                     \n",
       "2               10.686150      NaN     (64, 3, 3, 3)      NaN\n",
       "4               15.589570      NaN    (64, 64, 3, 3)      NaN\n",
       "7               17.629187      NaN   (128, 64, 3, 3)      NaN\n",
       "9               21.771776      NaN  (128, 128, 3, 3)      NaN\n",
       "12              23.527433      NaN  (256, 128, 3, 3)      NaN\n",
       "14              29.479052      NaN  (256, 256, 3, 3)      NaN\n",
       "16              29.830938      NaN  (256, 256, 3, 3)      NaN\n",
       "19              32.601643      NaN  (512, 256, 3, 3)      NaN\n",
       "21              40.713379      NaN  (512, 512, 3, 3)      NaN\n",
       "23              40.664497      NaN  (512, 512, 3, 3)      NaN\n",
       "26              41.603882      NaN  (512, 512, 3, 3)      NaN\n",
       "28              41.327408      NaN  (512, 512, 3, 3)      NaN\n",
       "30              40.439659      NaN  (512, 512, 3, 3)      NaN\n",
       "35             113.576752      NaN     (4096, 25088)      NaN\n",
       "38              58.012165      NaN      (4096, 4096)      NaN\n",
       "41              42.508610      NaN      (1000, 4096)      NaN"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
