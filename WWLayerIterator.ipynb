{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# WWLayerIterator\n",
    "\n",
    "This Notebook explains how to use the internal WeightWatcher Layer Iterators\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T23:51:16.544327Z",
     "start_time": "2019-03-05T23:51:16.531457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppress the powerlaw package warnings\n",
    "# \"powerlaw.py:700: RuntimeWarning: divide by zero encountered in true_divide\"\n",
    "# \"powerlaw.py:700: RuntimeWarning: invalid value encountered in true_divide\"\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-04T22:56:16-07:00\n",
      "\n",
      "CPython 3.8.5\n",
      "IPython 7.18.1\n",
      "\n",
      "compiler   : Clang 10.0.0 \n",
      "system     : Darwin\n",
      "release    : 17.7.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 12\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T00:16:43.249725Z",
     "start_time": "2019-03-06T00:16:43.240611Z"
    }
   },
   "source": [
    "### Import WeightWatcher\n",
    "\n",
    "set custom Logging at WARN Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import weightwatcher as ww\n",
    "import torchvision.models as models\n",
    "\n",
    "logger = logging.getLogger(ww.__name__)\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "ww.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How to create a WWLayerIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg11(pretrained=True)\n",
    "watcher = ww.WeightWatcher(model=model)\n",
    "ww_layer_iterator = watcher.make_layer_iterator(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Iterator lets you loop over WWLayer instances\n",
    "\n",
    "- The WWLayer instance (object) is a wrapper to the underlying framework layers\n",
    "\n",
    "- The intent is to only access the WWLayer instance and not the underlying framework methods\n",
    "\n",
    "- This lets weightwatcher apply different functions / transformations on each layer individually\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWLayer 2  None PYTORCH CONV2D  skipped False\n",
      "WWLayer 5  None PYTORCH CONV2D  skipped False\n",
      "WWLayer 8  None PYTORCH CONV2D  skipped False\n",
      "WWLayer 10  None PYTORCH CONV2D  skipped False\n",
      "WWLayer 13  None PYTORCH CONV2D  skipped False\n",
      "WWLayer 15  None PYTORCH CONV2D  skipped False\n",
      "WWLayer 18  None PYTORCH CONV2D  skipped False\n",
      "WWLayer 20  None PYTORCH CONV2D  skipped False\n",
      "WWLayer 25  None PYTORCH DENSE  skipped False\n",
      "WWLayer 28  None PYTORCH DENSE  skipped False\n",
      "WWLayer 31  None PYTORCH DENSE  skipped False\n"
     ]
    }
   ],
   "source": [
    "for ww_layer in ww_layer_iterator:\n",
    "    print(ww_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The WWLayerIterator constructor method takes\n",
    "\n",
    "- layers=[LAYER_ID, ...] to specify filters, as in the watcher.analyze(..,) and watcher.describe(...) methods\n",
    "\n",
    "- other parameters, like ww2x and channels, are specified in the parameters dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('weightwatcher') \n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "layers = [60]\n",
    "DEFAULT_PARAMS = {'glorot_fix': False, 'normalize':False, 'conv2d_norm':True, 'randomize': True, 'savefig':False, \n",
    "                  'rescale':True , 'deltaEs':False, 'intra':False, 'channels':None, 'conv2d_fft':False, \n",
    "                  'ww2x':False}\n",
    "\n",
    "params = DEFAULT_PARAMS\n",
    "ww_layer_iterator = watcher.make_layer_iterator(model=model, layers=layers, params=params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now only 1 layer is processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ww_layer in ww_layer_iterator:\n",
    "    print(ww_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WWLayer  Instances\n",
    "\n",
    "When a WWLayer instance is created, the weight matrices for the layer are extracted from the underlying framework tensor (i.e. layer.weights and layer.biases) and placed into WMats\n",
    "\n",
    "- WMats = [W,W,W,...] contains 1 or more W matrices, of the same shape NxM, N > M\n",
    "- evals: the *combined* evals for each layer\n",
    "- rf = 1 or (k)x(k)  the size of the 'receptive field'\n",
    "\n",
    "\n",
    "\n",
    "- layer_type =  an internal enum:    known layer types so far include:\n",
    "\n",
    "<pre>\n",
    "LAYER_TYPE.DENSE | LAYER_TYPE.CONV1D | LAYER_TYPE.CONV2D | LAYER_TYPE.FLATTENED |  LAYER_TYPE.EMBEDDING | LAYER_TYPE.NORM\n",
    "</pre>\n",
    "\n",
    "- channel_str:  string for channel type :  \"FIRST\" | \"LAST\" | \"UNKNOWN\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WeightWatcher Apply Methods\n",
    "\n",
    "#### The various apply_xxx() methods use basic metaprogramming to set additional instance variables\n",
    "\n",
    "\n",
    "- apply_filters()\n",
    "- apply_normalize_Wmats()\n",
    "- apply_esd()\n",
    "- apply_random_esd()\n",
    "- apply_plot_esd()\n",
    "- apply_fit_powerlaw()\n",
    "- apply_norm_metrics()\n",
    "- apply_plot_deltaEs()\n",
    "- apply_mp_fit()\n",
    "- apply_svdsmoothing()\n",
    "\n",
    "\n",
    "i.e the apply_esd() method runs SVD on all the WMats, then combines them into a single ESD\n",
    "\n",
    "<code>\n",
    "#\n",
    "    def apply_esd(self, ww_layer, params=AULT_PARAMS):\n",
    "            \"\"\"run full SVD on layer weight matrices, compute ESD on combined eigenvalues, combine all...\"\"\"\n",
    "#\n",
    "...\n",
    "#\n",
    "    ww_layer.evals = evals\n",
    "    ww_layer.add_column(\"has_esd\", True)\n",
    "    ww_layer.add_column(\"num_evals\", len(evals))\n",
    "    ww_layer.add_column(\"sv_max\", sv_max)\n",
    "    ww_layer.add_column(\"rank_loss\", rank_loss)\n",
    "    ww_layer.add_column(\"lambda_max\", np.max(evals))    \n",
    "#      \n",
    "    return ww_layer\n",
    "</code>       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are different Iterators for different ways of walking the Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weightwatcher.weightwatcher.WWLayerIterator"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ww_layer_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weightwatcher.weightwatcher import WWLayerIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weightwatcher.constants import LAYER_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WWStackedLayerIterator(WWLayerIterator):\n",
    "    \"\"\"Iterator variant that sticaks all weight matrics into a single WWLayer\"\"\"\n",
    "    from copy import deepcopy\n",
    "\n",
    "    def ww_stacked_iter_(self):\n",
    "        from copy import deepcopy\n",
    "        \n",
    "        # find the maximum dimensions so we can pad the matrices\n",
    "        ww_stacked_layer = None\n",
    "        Wmats = []\n",
    "        for ww_layer in self.ww_layer_iter_():\n",
    "            \n",
    "            # Here, Ijust lazizy copy an older layer\n",
    "            # really, we should creat the WWLayer using the init() constructor\n",
    "            if ww_stacked_layer is None:\n",
    "                ww_stacked_layer =  deepcopy(ww_layer)\n",
    "                ww_stacked_layer.the_type =  LAYER_TYPE.COMBINED\n",
    "                ww_stacked_layer.layer_id = 0  \n",
    "                ww_stacked_layer.name = \"Example Stacked Layer\"\n",
    "            Wmats.extend(ww_layer.Wmats)\n",
    "            \n",
    "        # Note: Here the matrices should be padded so that they are all the same width\n",
    "        #  ww_stacked_layer.Wmats = pad(Wmats)\n",
    "        # \n",
    "        \n",
    "        Nmax  = np.max([np.max(W.shape) for W in Wmats])\n",
    "        Mmin  = np.min([np.min(W.shape) for W in Wmats])\n",
    "                        \n",
    "        Wmats_padded = []\n",
    "        for W in Wmats:             \n",
    "            Height, Width = W.shape[0], W.shape[1]\n",
    "            if Height > Width:\n",
    "                W = W.T\n",
    "                Height, Width = W.shape[0], W.shape[1]\n",
    "                \n",
    "            W = np.pad(W, ((0, 0), (0, Nmax-Width)) ) \n",
    "            Wmats_padded.append(W)\n",
    "                \n",
    "        W_stacked = np.vstack(Wmats_padded)\n",
    "        N, M = W_stacked.shape[0],  W_stacked.shape[1]\n",
    "        if N < M:\n",
    "            W_stacked = W_stacked.T\n",
    "            N, M = W_stacked.shape[0],  W_stacked.shape[1]\n",
    "                    \n",
    "        ww_stacked_layer.Wmats = [W_stacked]\n",
    "    \n",
    "        # Then, the layer shape has to be set\n",
    "        # Just setting dummy variable here\n",
    "        ww_stacked_layer.N = N\n",
    "        ww_stacked_layer.M = M\n",
    "        ww_stacked_layer.rf = 1\n",
    "        #...\n",
    "    \n",
    "        \n",
    "        yield ww_stacked_layer\n",
    "                \n",
    "    def make_layer_iter_(self):\n",
    "        return self.ww_stacked_iter_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_iter = WWStackedLayerIterator(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWLayer 0  Example Stacked Layer PYTORCH COMBINED  skipped False\n"
     ]
    }
   ],
   "source": [
    "for layer in layer_iter:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29379, 25088)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.N, layer.M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice: The final matrix is quite large so the SVD will take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = layer.Wmats[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "svals = np.linalg.svd(W, compute_uv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=5000)\n",
    "svd.fit(W)\n",
    "svals = svd.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(svals, bins=100);\n",
    "plt.title(\"VGG11 SVs, all layer W's naively padded and stacked\")\n",
    "plt.xlabel(r\"Singular Values,  $\\sigma_{i}$\")\n",
    "plt.ylabel(r\"Density, $\\rho(\\sigma)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = svals*svals\n",
    "plt.hist(np.log10(evals), bins=100);\n",
    "plt.title(\"VGG11 ESD, all layer W's naively padded and stacked\")\n",
    "plt.xlabel(r\"Log EigenValues,  $\\log\\;\\lambda_{i}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(evals);\n",
    "plt.title(\"VGG11 Stacked ESD, log log plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
